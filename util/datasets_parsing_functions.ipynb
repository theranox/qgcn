{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting PyTorch Geometric Datasets from IAM Graph Dataset Repository's Graph Data Formats (in .txt forms)\n",
    "###### Run the below cell to generate pytorch datasets for training\n",
    "###### All raw dataset forms from IAM Graph Datasets Repository must be inside root/Datasets/Raw/*\n",
    "###### Functions in the cells reads and processes the raw datasets into geometric datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Graph Dataset with positional descriptors enabled: ON\n",
      "Dataset Name:  AIDS\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tedge_labels\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "AIDS: max_node_degree:  7 \n",
      "\n",
      "Dataset Name:  COIL-DEL\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tedge_labels\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "COIL-DEL: max_node_degree:  15 \n",
      "\n",
      "Dataset Name:  ENZYMES\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_attributes\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "\t\n",
      "Node positional descr exists: False\n",
      "ENZYMES: max_node_degree:  10 \n",
      "\n",
      "Dataset Name:  Fingerprint\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tedge_attributes\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "Fingerprint: max_node_degree:  4 \n",
      "\n",
      "Dataset Name:  FRANKENSTEIN\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: False\n",
      "FRANKENSTEIN: max_node_degree:  5 \n",
      "\n",
      "Dataset Name:  Letter-high\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "Letter-high: max_node_degree:  6 \n",
      "\n",
      "Dataset Name:  Letter-low\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "Letter-low: max_node_degree:  5 \n",
      "\n",
      "Dataset Name:  Letter-med\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: True\n",
      "Letter-med: max_node_degree:  5 \n",
      "\n",
      "Dataset Name:  MUTAG\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tedge_labels\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "\t\n",
      "Node positional descr exists: False\n",
      "MUTAG: max_node_degree:  5 \n",
      "\n",
      "Dataset Name:  Mutagenicity\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tedge_labels\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tlabel_readme\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "Node positional descr exists: False\n",
      "Mutagenicity: max_node_degree:  5 \n",
      "\n",
      "Dataset Name:  PROTEINS\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_attributes\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "\t\n",
      "\t\n",
      "Node positional descr exists: False\n",
      "PROTEINS: max_node_degree:  26 \n",
      "\n",
      "Dataset Name:  PROTEINS-Full\n",
      "Dataset Attributes: \n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_attributes\n",
      "\tnode_labels\n",
      "\tpos_exists\n",
      "\t\n",
      "\t\n",
      "Node positional descr exists: False\n",
      "PROTEINS-Full: max_node_degree:  26 \n",
      "\n",
      "Dataset Name:  Synthie\n",
      "Dataset Attributes: \n",
      "\t\n",
      "\t\n",
      "\tA\n",
      "\tgraph_indicator\n",
      "\tgraph_labels\n",
      "\tnode_attributes\n",
      "\tpos_exists\n",
      "Node positional descr exists: False\n",
      "Synthie: max_node_degree:  21 \n",
      "\n",
      "DONE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import statistics\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\"\"\"\n",
    "A graph is used to model pairwise relations (edges) between objects (nodes). A single graph in PyG is described by an instance of torch_geometric.data.Data, which holds the following attributes by default:\n",
    "Ref: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html\n",
    "data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" Extracts Graph Properties from .txt files and creates a Raw Dict Dataset structure\"\"\"\n",
    "def extract_geometric_dataset(dataset_name: str, rel_path: str, pde: bool = False):\n",
    "    print(\"Dataset Name: \", dataset_name)\n",
    "    all_data_files = [ str((\"_\".join(data.split(\"_\")[1:])).split(\".\")[0]) for data in os.listdir(rel_path)]\n",
    "    print(\"Dataset Attributes: \")\n",
    "    for x in all_data_files:\n",
    "        print(f\"\\t{x}\")\n",
    "\n",
    "    edges_path = os.path.join(rel_path, f'{dataset_name}_A.txt') if 'A' in all_data_files else None\n",
    "    edge_attrib_path = os.path.join(rel_path, f'{dataset_name}_edge_attributes.txt') if 'edge_attributes' in all_data_files else None \n",
    "    node_attrib_path = os.path.join(rel_path, f'{dataset_name}_node_attributes.txt') if 'node_attributes' in all_data_files else None \n",
    "    node_labels_path = os.path.join(rel_path, f'{dataset_name}_node_labels.txt') if 'node_labels' in all_data_files else None\n",
    "    graph_indicator_path = os.path.join(rel_path, f'{dataset_name}_graph_indicator.txt') if 'graph_indicator' in all_data_files else None\n",
    "    graph_classes_path = os.path.join(rel_path, f'{dataset_name}_graph_labels.txt') if 'graph_labels' in all_data_files else None\n",
    "    node_pos_descr_exists_path = os.path.join(rel_path, f'{dataset_name}_pos_exists.txt') if 'pos_exists' in all_data_files else None\n",
    "\n",
    "    # Read to determine whether positional descriptors exist\n",
    "    assert not isinstance(node_pos_descr_exists_path, type(None)), \"Every data to be processed must define *_pos_exists.txt to define whether positional descriptors exist\"\n",
    "    node_pos_descr_exists = False\n",
    "    with open(node_pos_descr_exists_path, \"r\") as data_file:\n",
    "        node_pos_descr_exists = int(str(data_file.readlines()[0]).split(\",\")[-1].strip()) == 1\n",
    "        print(f\"Node positional descr exists: {node_pos_descr_exists}\")\n",
    "\n",
    "    # Loop and read in data files\n",
    "    # Graph Indicator\n",
    "    assert not isinstance(graph_indicator_path, type(None)), \"Graph Indicator must not be None\"\n",
    "    graph_indicator_dict = { \"config\": None, \"indicators\": None, \"num_graphs\": -1 }\n",
    "    with open(graph_indicator_path, \"r\") as data_file:\n",
    "        graph_indicator_config = {} # e.g. 1: {start:<int>, end:<int>}\n",
    "        indicator_ordered_list = [] # used to run through the other lists for read\n",
    "        for i, graph_indicator in enumerate(data_file.readlines()):\n",
    "            if int(graph_indicator) not in graph_indicator_config:\n",
    "                # create obj for this\n",
    "                indicator_obj = {\"start\": i, \"end\": i }\n",
    "                graph_indicator_config[int(graph_indicator)] = indicator_obj\n",
    "                indicator_ordered_list.append(int(graph_indicator))\n",
    "            else:\n",
    "                graph_indicator_config[int(graph_indicator)][\"end\"] += 1\n",
    "        # create graph indic. struct\n",
    "        graph_indicator_dict[\"config\"]     = graph_indicator_config\n",
    "        graph_indicator_dict[\"indicators\"] = indicator_ordered_list\n",
    "        graph_indicator_dict[\"num_graphs\"] = len(indicator_ordered_list)\n",
    "\n",
    "    # Read the graph labels and assign labels to graphs\n",
    "    assert not isinstance(graph_classes_path, type(None)), \"Graph Labels must not be None\"\n",
    "    with open(graph_classes_path, \"r\") as data_file:\n",
    "        all_graph_labels = [ int(graph_label_str) for graph_label_str in data_file.readlines() ]\n",
    "        min_graph_label = min(all_graph_labels)\n",
    "        # re-normalized to zero-indexed\n",
    "        renormed_labels = [ int(label - min_graph_label) for label in all_graph_labels ]\n",
    "        # Labels remapped -> contiguous array\n",
    "        remapped_labels = { label: idx for idx, label in enumerate(set(renormed_labels)) }\n",
    "        # save graph labels in graph configs\n",
    "        for graph_indicator, graph_label in zip(graph_indicator_dict[\"indicators\"], renormed_labels):\n",
    "            graph_indicator_dict[\"config\"][graph_indicator]['label'] = int(remapped_labels[graph_label])\n",
    "\n",
    "    # Use config to extract out the node attrib details\n",
    "    \"\"\"\n",
    "    Node Attributes: E.g. below\n",
    "        - AIDS: \n",
    "            Node labels:\t\t[symbol]\n",
    "            Node attributes:\t[chem, charge, x, y]\n",
    "          Fingerprints:\n",
    "            Node attributes:\t[x, y]\n",
    "    Used attributes:\n",
    "        - Zip node labels together with node attributes -> New node attributes\n",
    "        - Remove last 2 node attributes and save that as node_pos\n",
    "        - Remaining goes into node attributes list  \n",
    "    \"\"\"\n",
    "    assert not (isinstance(node_attrib_path, type(None)) and isinstance(node_labels_path, type(None))), \"Node Attributes must not be None\"\n",
    "    all_node_attribs = [] # holds all node attributes\n",
    "    # Extract node attributes\n",
    "    if node_attrib_path is not None:\n",
    "        with open(node_attrib_path, \"r\") as data_file:\n",
    "            all_node_attribs = [ [ float(x.strip()) for x in line.split(\",\")] for line in data_file.readlines() ]\n",
    "    # Extract node labels - as node attributes\n",
    "    if node_labels_path is not None:\n",
    "        with open(node_labels_path, \"r\") as node_labels_data_file:\n",
    "            all_node_labels = [ [ float(x.strip()) for x in line.split(\",\")] for line in node_labels_data_file.readlines() ]\n",
    "            # extend the primary list with node labels\n",
    "            if len(all_node_attribs) > 0:\n",
    "                assert len(all_node_attribs) == len(all_node_labels), \"Total num. of node features and node labels must match\"\n",
    "                all_node_attribs = [ [ *all_node_labels[i], *orig_node_attribs_lst ] for i, orig_node_attribs_lst in enumerate(all_node_attribs) ]\n",
    "            else:\n",
    "                # Treat Node labels as Node features\n",
    "                all_node_attribs = all_node_labels\n",
    "\n",
    "    # Read in edge attributes\n",
    "    assert not isinstance(edges_path, type(None)), \"Edge Attributes must not be None\"\n",
    "    with open(edges_path, \"r\") as edges_data_file:\n",
    "        edge_connectivities = [ [int(x.strip()) for x in line.split(\",\")] for line in edges_data_file.readlines() ]\n",
    "        edge_connectivity_adjacency_list_map = {}\n",
    "        for in_node, out_node in edge_connectivities:\n",
    "            if in_node not in edge_connectivity_adjacency_list_map:\n",
    "                edge_connectivity_adjacency_list_map[in_node] = set( [out_node] )\n",
    "            else:\n",
    "                edge_connectivity_adjacency_list_map[in_node].add(out_node)\n",
    "\n",
    "        # Add self loops\n",
    "        for node_label in range(1, len(all_node_attribs)+1):\n",
    "            if node_label in edge_connectivity_adjacency_list_map:\n",
    "                edge_connectivity_adjacency_list_map[node_label].add(node_label)\n",
    "            else:\n",
    "                edge_connectivity_adjacency_list_map[node_label] = set( [ node_label ] )\n",
    "\n",
    "        # # assertions\n",
    "        assert len(edge_connectivity_adjacency_list_map.keys()) == len(all_node_attribs), \"Nodes in edge connectivities must match nodes in attribute descriptors file\"\n",
    "\n",
    "        # Extract the node degree as an attribute on the nodes themselves\n",
    "        for i in range(len(all_node_attribs)):\n",
    "            node_label = i + 1\n",
    "            node_degree = len(edge_connectivity_adjacency_list_map[node_label])\n",
    "            all_node_attribs[i].insert(0, float(node_degree))\n",
    "\n",
    "        # Build the edge features if they exist\n",
    "        edge_feats_map = None if edge_attrib_path is None else {}\n",
    "        if edge_attrib_path is not None:\n",
    "            with open(edge_attrib_path, \"r\") as edges_attr_data_file:\n",
    "                edge_attrs = [ [float(x.strip()) for x in line.split(\",\")] for line in edges_attr_data_file.readlines() ]\n",
    "                assert len(edge_attrs) == len(edge_connectivities), \"Edge Attributes length must match num edges in Edge Index (Edges Path) data type\"\n",
    "                # Create a mapping from each edge to each edge feature\n",
    "                num_edge_feats = -1\n",
    "                for idx, (in_node, out_node) in enumerate(edge_connectivities):\n",
    "                    if num_edge_feats == -1: num_edge_feats = len(edge_attrs[idx])\n",
    "                    assert num_edge_feats == len(edge_attrs[idx]), \"Edge Attributes must be same dimension\"\n",
    "                    edge_feats_map[(in_node, out_node)] = edge_attrs[idx]\n",
    "                # Handle case for self-loops [feats = [0] if self-loop doesn't exist]\n",
    "                for i in range(len(all_node_attribs)):\n",
    "                    node_label = i + 1\n",
    "                    self_loop_edge = (node_label, node_label)\n",
    "                    if self_loop_edge not in edge_feats_map:\n",
    "                        edge_feats_map[self_loop_edge] = num_edge_feats * [0.0] # Null all features\n",
    "\n",
    "    # Assert that node attributes have length >= 2\n",
    "    # NOTE: idx=0 -> Node Degree\n",
    "    #       idx=1 -> Node Label\n",
    "    #       ...\n",
    "    assert all([ len(node_attr) >= 2 for node_attr in all_node_attribs ]), \"All nodes must have at least 2 features, idx=0 (Node Degree) and idx=1 (Node label)\"\n",
    "\n",
    "    # Separate node position indicators from other attributes\n",
    "    # node_position_descr = [ node_attrib[-2:] if node_pos_descr_exists else node_attrib[:2] for node_attrib in all_node_attribs ]\n",
    "    extract_pos_descr = lambda node_attr: [ statistics.mean(node_attr[:len(node_attr)//2]), statistics.mean(node_attr[len(node_attr)//2:]) ]\n",
    "    node_position_descr = [ node_attrib[-2:] if node_pos_descr_exists else extract_pos_descr(node_attrib) for node_attrib in all_node_attribs ]\n",
    "    node_attrib_descr   = [ node_attrib[:-2] for node_attrib in all_node_attribs ] if (node_pos_descr_exists and (not pde)) else all_node_attribs\n",
    "    assert len(node_position_descr) == len(node_attrib_descr), \"Node attributes and position descriptors must be same\"\n",
    "\n",
    "    # NOTE:\n",
    "    #   Goal: Build the PyTorch object for a graph dataset\n",
    "    # Loop through graphs config and assign nodes and edge attributes\n",
    "    graphs = []\n",
    "    max_node_degree = 0\n",
    "    for graph_indicator in graph_indicator_dict[\"indicators\"]:\n",
    "        indicator_start = graph_indicator_dict[\"config\"][graph_indicator][\"start\"]\n",
    "        indicator_end = graph_indicator_dict[\"config\"][graph_indicator][\"end\"]\n",
    "        # base_attribs = {  }\n",
    "        # pde_optional_attrib = { \"pos\": [] }\n",
    "        # graph_attribs = { **base_attribs, **(pde_optional_attrib if pde else {}) }\n",
    "        graph_attribs = { \"x\": [], \"edge_index\": [], \"pos\": [], \"y\": [] }\n",
    "        if edge_feats_map is not None: graph_attribs.update({ \"edge_attr\": [] }) \n",
    "        edge_indices_coo_format = [[], []]\n",
    "        for i in range(indicator_start, indicator_end + 1):\n",
    "            node_label = i + 1 # ---> nodes file line number\n",
    "            graph_attribs[\"x\"].append(node_attrib_descr[i])\n",
    "            graph_attribs[\"pos\"].append(node_position_descr[i])\n",
    "            # if \"x\" in graph_attribs: graph_attribs[\"x\"].append(node_attrib_descr[i])\n",
    "            # if \"pos\" in graph_attribs: graph_attribs[\"pos\"].append(node_position_descr[i])\n",
    "            # add the edge indices\n",
    "            target_nodes = edge_connectivity_adjacency_list_map[node_label]\n",
    "            max_node_degree = max_node_degree if max_node_degree >= len(target_nodes) else len(target_nodes)\n",
    "            edge_indices_coo_format[0].extend( len(target_nodes) * [node_label] )\n",
    "            edge_indices_coo_format[1].extend( list(target_nodes))\n",
    "            # compile the edge attributes (if they exist)\n",
    "            if edge_feats_map is not None:\n",
    "                for target_node in target_nodes:\n",
    "                    graph_attribs[\"edge_attr\"].append(edge_feats_map[(node_label, target_node)])\n",
    "        \n",
    "        # re-normalize edge_indices \n",
    "        min_node_id = min(edge_indices_coo_format[0])\n",
    "        edge_indices_coo_format[0] = [ (node_id - min_node_id) for node_id in edge_indices_coo_format[0] ]\n",
    "        edge_indices_coo_format[1] = [ (node_id - min_node_id) for node_id in edge_indices_coo_format[1] ]\n",
    "\n",
    "        # Add graph edge indices and label\n",
    "        graph_attribs[\"edge_index\"] = edge_indices_coo_format\n",
    "        graph_attribs[\"y\"] = [ graph_indicator_dict[\"config\"][graph_indicator]['label'] ] # pytorch geometric expects this format\n",
    "        # Add to graphs pile\n",
    "        graphs.append(graph_attribs)\n",
    "    \n",
    "    # Assertions\n",
    "    assert len(graphs) == len(graph_indicator_dict[\"indicators\"]), \"Num graphs must match Num graph indicators\"\n",
    "\n",
    "    # Print out max node degree\n",
    "    print(f\"{dataset_name}: max_node_degree: \", max_node_degree, \"\\n\")\n",
    "\n",
    "    # Return graphs\n",
    "    return graphs\n",
    "\n",
    "\"\"\" Converts Raw Dataset structured in GEO format to PyTorch type and Geometric Data Type \"\"\"\n",
    "def convert_to_pytorch_graph_dataset(dataset: list):\n",
    "    data_attrib_typing = {\n",
    "        'x': torch.FloatTensor,\n",
    "        'y': torch.LongTensor,\n",
    "        'pos': torch.FloatTensor,\n",
    "        'edge_attr': torch.FloatTensor,\n",
    "        'edge_index': torch.LongTensor\n",
    "    }\n",
    "\n",
    "    # Loop through and init data\n",
    "    torch_geo_dataset = []\n",
    "    for data_item in dataset:\n",
    "        # Init empty Geo Graph Data\n",
    "        geo_graph_data_elem = Data()\n",
    "        geo_graph_data_allowed_keys = list(Data.__dict__.keys())\n",
    "        assert all([ raw_data_key in geo_graph_data_allowed_keys  for raw_data_key in list(data_item.keys()) ]), \"Raw Graph Data structure to be parsed into PyTorch Geometric struct contains keys not supported\"\n",
    "        # populate with fields\n",
    "        for data_attrib_key, data_attrib_val in data_item.items():\n",
    "            data_attrib_type = data_attrib_typing[data_attrib_key]\n",
    "            data_attrib_val_geo_torch = torch.tensor(data_attrib_val).type(data_attrib_type)\n",
    "            if hasattr(geo_graph_data_elem, data_attrib_key):\n",
    "                setattr(geo_graph_data_elem, data_attrib_key, data_attrib_val_geo_torch)\n",
    "        # Add to collection\n",
    "        torch_geo_dataset.append(geo_graph_data_elem)\n",
    "\n",
    "    # return new dataset\n",
    "    return torch_geo_dataset\n",
    "\n",
    "\"\"\" Saves the PyTorch Geometric Dataset to Generated/ dir \"\"\"\n",
    "def save_graph_datasets(positional_descriptors_enabled: bool = False):\n",
    "    print(f\"Generating Graph Dataset with positional descriptors enabled: { 'ON' if positional_descriptors_enabled else 'OFF'}\")\n",
    "    # Save pickle file objects of the datasets required\n",
    "    parent_dirpath = os.path.join(\".\", \"Dataset\", \"Raw\")\n",
    "    gen_folder_name = \"Generated\"\n",
    "    for dir in os.listdir(parent_dirpath):\n",
    "        if dir != gen_folder_name and os.path.isdir(os.path.join(parent_dirpath, dir)):\n",
    "            rel_path = os.path.join(parent_dirpath, dir)\n",
    "            dataset = extract_geometric_dataset(dataset_name=dir, rel_path=rel_path, pde=positional_descriptors_enabled)\n",
    "            geo_graph_dataset = convert_to_pytorch_graph_dataset(dataset)\n",
    "            # Save a pickle object of this file\n",
    "            filename = f\"{dir.strip().lower()}_graph_data_pde={ 'yes' if positional_descriptors_enabled else 'no' }.pkl\"\n",
    "            if not os.path.exists(os.path.join(parent_dirpath, gen_folder_name)):\n",
    "                os.mkdir(os.path.join(parent_dirpath, gen_folder_name))\n",
    "            # Add generated file\n",
    "            store_file_path = os.path.join(parent_dirpath, gen_folder_name, filename)\n",
    "            with open(store_file_path, 'wb') as handle:\n",
    "                pickle.dump(geo_graph_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"DONE \\n\")\n",
    "\n",
    "\"\"\" Important helper function to read generated Torch Dataset \"\"\"\n",
    "def read_dataset(dataset_name=\"AIDS\", pde: bool = False):\n",
    "    # Save pickle file objects of the datasets required\n",
    "    parent_dirpath = os.path.join(\".\", \"Dataset\", \"Raw\", \"Generated\")\n",
    "    filename = f\"{dataset_name.strip().lower()}_graph_data_pde={ 'yes' if pde else 'no' }.pkl\"\n",
    "    rel_dataset_path = os.path.join(parent_dirpath, filename)\n",
    "    with open(rel_dataset_path, \"rb\") as pickle_file_handle:\n",
    "        loaded_dataset = pickle.load(pickle_file_handle)\n",
    "    # inspect dataset\n",
    "    return loaded_dataset\n",
    "\n",
    "\"\"\"\n",
    "Dataset Characteristics:\n",
    "    - Self loops available = YES\n",
    "    - Node attributes have node degree in addition to primary attributes\n",
    "    - Edge labels and Edge attributes are unused\n",
    "    - Node mux var: pde: positional descriptors enabled\n",
    "        pde == yes:\n",
    "            We separate out positional descriptors into data.pos\n",
    "        pde == no:\n",
    "            Positional descriptors are removed for geo graph data elem\n",
    "\"\"\"\n",
    "save_graph_datasets(positional_descriptors_enabled = False)\n",
    "save_graph_datasets(positional_descriptors_enabled = True)\n",
    "############################ EOF #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating different dataset splits for the datasets +\n",
    "## Parsing into a struct that experiment.py can consume\n",
    "###### experiment.py struct format:\n",
    "###### struct = { \"raw\": {\"x_train_data\": [], \"y_train_data\": [], \"x_test_data\" : [], \"y_test_data\" : []}, \"geometric\": {\"qgcn_train_data\":  [], \"qgcn_test_data\" : [], \"sgcn_train_data\": [], \"sgcn_test_data\" :  []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDS - Classes splits:  {0: 400, 1: 1600} \n",
      "\n",
      "COIL-DEL - Classes splits:  {14: 39, 99: 39, 18: 39, 3: 39, 57: 39, 87: 39, 38: 39, 24: 39, 63: 39, 98: 39, 46: 39, 52: 39, 2: 39, 89: 39, 78: 39, 15: 39, 73: 39, 79: 39, 29: 39, 16: 39, 6: 39, 12: 39, 51: 39, 47: 39, 39: 39, 86: 39, 31: 39, 88: 39, 10: 39, 61: 39, 22: 39, 35: 39, 1: 39, 42: 39, 27: 39, 93: 39, 23: 39, 36: 39, 75: 39, 58: 39, 43: 39, 91: 39, 26: 39, 60: 39, 83: 39, 33: 39, 56: 39, 55: 39, 72: 39, 49: 39, 13: 39, 84: 39, 5: 39, 90: 39, 20: 39, 45: 39, 69: 39, 74: 39, 82: 39, 7: 39, 28: 39, 70: 39, 94: 39, 76: 39, 9: 39, 50: 39, 71: 39, 80: 39, 25: 39, 95: 39, 40: 39, 21: 39, 34: 39, 64: 39, 11: 39, 32: 39, 96: 39, 41: 39, 19: 39, 97: 39, 8: 39, 66: 39, 4: 39, 59: 39, 68: 39, 92: 39, 81: 39, 62: 39, 17: 39, 0: 39, 65: 39, 48: 39, 30: 39, 37: 39, 77: 39, 53: 39, 67: 39, 85: 39, 54: 39, 44: 39} \n",
      "\n",
      "ENZYMES - Classes splits:  {5: 100, 4: 100, 0: 100, 1: 100, 2: 100, 3: 100} \n",
      "\n",
      "Fingerprint - Classes splits:  {0: 369, 1: 136, 2: 496, 3: 75, 4: 396, 5: 368, 6: 134, 7: 4, 8: 105, 9: 18, 10: 20, 11: 20, 12: 2, 13: 4, 14: 2} \n",
      "\n",
      "FRANKENSTEIN - Classes splits:  {1: 2401, 0: 1936} \n",
      "\n",
      "Letter-high - Classes splits:  {0: 150, 1: 150, 2: 150, 3: 150, 4: 150, 5: 150, 6: 150, 7: 150, 8: 150, 9: 150, 10: 150, 11: 150, 12: 150, 13: 150, 14: 150} \n",
      "\n",
      "Letter-low - Classes splits:  {0: 150, 1: 150, 2: 150, 3: 150, 4: 150, 5: 150, 6: 150, 7: 150, 8: 150, 9: 150, 10: 150, 11: 150, 12: 150, 13: 150, 14: 150} \n",
      "\n",
      "Letter-med - Classes splits:  {0: 150, 1: 150, 2: 150, 3: 150, 4: 150, 5: 150, 6: 150, 7: 150, 8: 150, 9: 150, 10: 150, 11: 150, 12: 150, 13: 150, 14: 150} \n",
      "\n",
      "MUTAG - Classes splits:  {1: 125, 0: 63} \n",
      "\n",
      "Mutagenicity - Classes splits:  {0: 2401, 1: 1936} \n",
      "\n",
      "PROTEINS - Classes splits:  {0: 663, 1: 450} \n",
      "\n",
      "PROTEINS-Full - Classes splits:  {0: 663, 1: 450} \n",
      "\n",
      "Synthie - Classes splits:  {3: 90, 1: 107, 2: 110, 0: 93} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from copy import copy, deepcopy\n",
    "# TODO: Create test and train datasets: Example showcased below\n",
    "    # AIDs: [Binary]\n",
    "    #               [100, 20] & [1000, 200] & [1600, 400]\n",
    "    #                50s 10s     500s 100s     800s 200s\n",
    "    # Fingerprints: [15 Classes]\n",
    "    #               [150, 30] & [1050, 150] & [1650, 450]\n",
    "    #                10s  2s     70s  10s      110s  30s\n",
    "# NOTE: Make sure for small dataset split, num classes are balanced out\n",
    "# NOTE: Randomize dataset for all split\n",
    "\n",
    "def read_dataset(dataset_name=\"AIDS\", pde: bool = False):\n",
    "    # Save pickle file objects of the datasets required\n",
    "    parent_dirpath = os.path.join(\".\", \"Dataset\", \"Raw\", \"Generated\")\n",
    "    filename = f\"{dataset_name.strip().lower()}_graph_data_pde={ 'yes' if pde else 'no' }.pkl\"\n",
    "    rel_dataset_path = os.path.join(parent_dirpath, filename)\n",
    "    with open(rel_dataset_path, \"rb\") as pickle_file_handle:\n",
    "        loaded_dataset = pickle.load(pickle_file_handle)\n",
    "    # inspect dataset\n",
    "    return loaded_dataset\n",
    "\n",
    "def order_dataset_into_classes(loaded_dataset: list):\n",
    "    classes_dict = {}\n",
    "    for data in loaded_dataset:\n",
    "        # classes_dict[data.y.item()] = [ *classes_dict.get(data.y.item(), []),  data ]\n",
    "        if data.y.item() in classes_dict: classes_dict[data.y.item()].append(data)\n",
    "        else: classes_dict[data.y.item()] = [ data ]\n",
    "    return classes_dict\n",
    "\n",
    "def find_dataset_splits(dataset_name: str = \"AIDS\", pde: bool = False):\n",
    "    splits = {\n",
    "        \"aids\": [\n",
    "            [100, 20], \n",
    "            [1000, 200], \n",
    "            [1600, 400]\n",
    "        ],\n",
    "        \"coil-del\": [\n",
    "            [500,  100], \n",
    "            [1000, 200], \n",
    "            [3200, 700]\n",
    "        ],\n",
    "        \"enzymes\": [\n",
    "            [480,  120]\n",
    "        ],\n",
    "        \"fingerprint\": [\n",
    "            [150, 30], \n",
    "            [1050, 150], \n",
    "            [1650, 450],\n",
    "            [1650, 495]\n",
    "        ],\n",
    "        \"frankenstein\": [\n",
    "            [100, 20], \n",
    "            [1000, 200], \n",
    "            [2000, 500],\n",
    "            [3400, 900]\n",
    "        ],\n",
    "        \"letter-low\": [\n",
    "            [150, 30], \n",
    "            [1050, 150], \n",
    "            [1650, 450],\n",
    "            [1725, 525]\n",
    "        ],\n",
    "        \"letter-med\": [\n",
    "            [150, 30], \n",
    "            [1050, 150], \n",
    "            [1650, 450],\n",
    "            [1725, 525]\n",
    "        ],\n",
    "        \"letter-high\": [\n",
    "            [150, 30], \n",
    "            [1050, 150], \n",
    "            [1650, 450],\n",
    "            [1725, 525]\n",
    "        ],\n",
    "        \"mutag\": [\n",
    "            [100, 20],\n",
    "            [148, 40]\n",
    "        ],\n",
    "        \"mutagenicity\": [\n",
    "            [100, 20], \n",
    "            [1000, 200], \n",
    "            [2000, 500],\n",
    "            [3400, 900]\n",
    "        ],\n",
    "        \"proteins\": [\n",
    "            [100, 20], \n",
    "            [850, 250], \n",
    "            [1000, 100]\n",
    "        ],\n",
    "        \"proteins-full\": [\n",
    "            [100, 20], \n",
    "            [850, 250], \n",
    "            [1000, 100]\n",
    "        ],\n",
    "        \"synthie\": [\n",
    "            [100, 20],\n",
    "            [320, 80]\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    \"\"\"\"\n",
    "    AIDS: \n",
    "        num_classes: 2\n",
    "        max_node_degree:  7\n",
    "        node_features: pde_on-6, pde_off-4\n",
    "        total_dataset: 2000     \n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:1000_test:200\n",
    "            - train:1600_test:400\n",
    "    COIL-DEL: \n",
    "        num_classes: 100\n",
    "        max_node_degree:  15\n",
    "        node_features: pde_on-3, pde_off-1\n",
    "        total_dataset: 3900\n",
    "        splits:\n",
    "            - train:500_test:100\n",
    "            - train:1000_test:200\n",
    "            - train:3200_test:700\n",
    "    ENZYMES:\n",
    "        num_classes: 6\n",
    "        max_node_degree:  10\n",
    "        node_features: pde_on-20, pde_off-20\n",
    "        total_dataset: 600\n",
    "        splits:\n",
    "            - train:480_test:120\n",
    "    Fingerprint:\n",
    "        num_classes: 15\n",
    "        max_node_degree: 4\n",
    "        node_features: pde_on-3, pde_off-3\n",
    "        num_edge_feats: 2\n",
    "        total_dataset: 2149\n",
    "        splits:\n",
    "            - train:150_test:30\n",
    "            - train:1050_test:150\n",
    "            - train:1650_test:450\n",
    "            - train:1650_test:495\n",
    "    FRANKENSTEIN:\n",
    "        num_classes: 2\n",
    "        max_node_degree: 4\n",
    "        node_features: pde_on-781, pde_off-779\n",
    "        total_dataset: 4337\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:1000_test:200\n",
    "            - train:2000_test:500\n",
    "            - train:3400_test:900\n",
    "    Letter-high: \n",
    "        num_classes: 15\n",
    "        max_node_degree:  6\n",
    "        node_features: pde_on-3, pde_off-1\n",
    "        total_dataset: 2250\n",
    "        splits:\n",
    "            - train:150_test:30\n",
    "            - train:1050_test:150\n",
    "            - train:1650_test:450\n",
    "            - train:1725_test:525\n",
    "    Letter-low:\n",
    "        num_classes: 15\n",
    "        max_node_degree:  5\n",
    "        node_features: pde_on-3, pde_off-1\n",
    "        total_dataset: 2250\n",
    "        splits:\n",
    "            - train:150_test:30\n",
    "            - train:1050_test:150\n",
    "            - train:1650_test:450\n",
    "            - train:1725_test:525\n",
    "    Letter-med: \n",
    "        num_classes: 15\n",
    "        max_node_degree:  5\n",
    "        node_features: pde_on-3, pde_off-1\n",
    "        total_dataset: 2250\n",
    "        splits:\n",
    "            - train:150_test:30\n",
    "            - train:1050_test:150\n",
    "            - train:1650_test:450\n",
    "            - train:1725_test:525\n",
    "    MUTAG:\n",
    "        num_classes: 2\n",
    "        max_node_degree: 5\n",
    "        node_features: pde_on-2, pde_off-2\n",
    "        total_dataset: 188\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:148_test:40\n",
    "    Mutagenicity:\n",
    "        num_classes: 2\n",
    "        max_node_degree:  5\n",
    "        node_features: pde_on-2, pde_off-2\n",
    "        total_dataset: 4337\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:1000_test:200\n",
    "            - train:2000_test:500\n",
    "            - train:3400_test:900\n",
    "    PROTEINS:\n",
    "        num_classes: 2\n",
    "        max_node_degree:  26\n",
    "        node_features: pde_on-3, pde_off-3\n",
    "        total_dataset: 1113\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:850_test:250\n",
    "            - train:1000_test:100\n",
    "    PROTEINS-Full:\n",
    "        num_classes: 2\n",
    "        max_node_degree:  26\n",
    "        node_features: pde_on-31, pde_off-31\n",
    "        total_dataset: 1113\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:850_test:250\n",
    "            - train:1000_test:100\n",
    "    Synthie:\n",
    "        num_classes: 4\n",
    "        max_node_degree:  21\n",
    "        node_features: pde_on-16, pde_off-16\n",
    "        total_dataset: 400\n",
    "        splits:\n",
    "            - train:100_test:20\n",
    "            - train:320_test:80\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_split = splits[dataset_name.strip().lower()]\n",
    "    dataset = read_dataset(dataset_name=dataset_name, pde=pde)\n",
    "    classes_dict = order_dataset_into_classes(dataset)\n",
    "    classes_size = { it: len(val) for it, val in classes_dict.items() }\n",
    "\n",
    "    print(f\"{dataset_name} - Classes splits: \", { k: len(v) for k, v in classes_dict.items() }, \"\\n\")\n",
    "\n",
    "    # Mapping of how to partition data\n",
    "    dataset_mapping_splits = {}\n",
    "\n",
    "    # Split into the train and test size\n",
    "    for train_size, test_size in dataset_split:\n",
    "        # Determine if we meet split targets\n",
    "        classes_size_cpy = classes_size.copy()\n",
    "        per_class_train_size = math.floor(train_size / len(classes_size_cpy.keys()))\n",
    "        per_class_test_size  = math.ceil(test_size / len(classes_size_cpy.keys()))\n",
    "        total_per_class_contrib = per_class_test_size + per_class_train_size\n",
    "        train_sf = (per_class_train_size / total_per_class_contrib)\n",
    "        test_sf = (per_class_test_size / total_per_class_contrib)\n",
    "        split_classes_col_dict = {  class_id: [0, 0] for class_id in classes_size_cpy }\n",
    "        remaining = (train_size + test_size) - sum([ sum(split) for split in split_classes_col_dict.values() ])\n",
    "        while remaining > 0:\n",
    "            # For classes that have more data left, add more to train and test\n",
    "            non_zero_classes = list(filter(lambda x: classes_size_cpy[x] > 0, classes_size_cpy.keys()))\n",
    "            filtered_classes = { class_id: classes_size_cpy[class_id] for class_id in non_zero_classes }\n",
    "            size_per_class = remaining / len(non_zero_classes)\n",
    "            add_train_size = math.floor(size_per_class * train_sf)\n",
    "            add_test_size  = math.ceil(size_per_class * test_sf)\n",
    "\n",
    "            # Helper functions for determining whether a class can contribute more training / testing data\n",
    "            train_data_can_accommodate = lambda x: math.floor(classes_size[x] * train_sf) >= (split_classes_col_dict[x][0] + add_train_size)\n",
    "            test_data_can_accommodate  = lambda x: math.ceil(classes_size[x] * test_sf)   >= (split_classes_col_dict[x][1] + add_test_size)\n",
    "            # redist = len(list(filter(lambda x: train_data_can_accommodate(x) and test_data_can_accommodate(x), filtered_classes.keys()))) != len(filtered_classes)\n",
    "\n",
    "            # Add data from largest classes\n",
    "            if math.floor(size_per_class) == 0:\n",
    "                sorted_redist_list = sorted(non_zero_classes, key=lambda x: filtered_classes[x])\n",
    "                filtered_redist_list = []\n",
    "                rolling_sum = 0\n",
    "                for class_id in sorted_redist_list:\n",
    "                    if rolling_sum < remaining:\n",
    "                        filtered_redist_list.append(class_id)\n",
    "                        rolling_sum += filtered_classes[class_id]\n",
    "                    else:\n",
    "                        break\n",
    "                # redist from filtered class ids\n",
    "                for class_id in filtered_redist_list:\n",
    "                    num_data = filtered_classes[class_id]\n",
    "                    if num_data < remaining:\n",
    "                        delta_train_size = math.floor(num_data * train_sf)\n",
    "                        delta_test_size = math.ceil(num_data * test_sf)\n",
    "                        split_classes_col_dict[class_id][0] += delta_train_size\n",
    "                        split_classes_col_dict[class_id][1] += delta_test_size\n",
    "                        classes_size_cpy[class_id] -= (delta_train_size + delta_test_size)\n",
    "                        remaining -= (delta_train_size + delta_test_size)\n",
    "                    else:\n",
    "                        delta_train_size = math.floor(remaining * train_sf)\n",
    "                        delta_test_size = math.ceil(remaining * test_sf)\n",
    "                        split_classes_col_dict[class_id][0] += delta_train_size\n",
    "                        split_classes_col_dict[class_id][1] += delta_test_size\n",
    "                        remaining -= (delta_train_size + delta_test_size)\n",
    "                        break\n",
    "            else:\n",
    "                # Redistribute data from other classes\n",
    "                for class_id in filtered_classes:\n",
    "                    if train_data_can_accommodate(class_id) and test_data_can_accommodate(class_id):\n",
    "                        split_classes_col_dict[class_id][0] += add_train_size\n",
    "                        split_classes_col_dict[class_id][1] += add_test_size\n",
    "                        classes_size_cpy[class_id] -= (add_train_size + add_test_size) \n",
    "                    else:\n",
    "                        delta_train_size = math.floor(filtered_classes[class_id] * train_sf)\n",
    "                        delta_test_size  = math.ceil(filtered_classes[class_id] * test_sf)\n",
    "                        split_classes_col_dict[class_id][0] += delta_train_size\n",
    "                        split_classes_col_dict[class_id][1] += delta_test_size\n",
    "                        classes_size_cpy[class_id] -= (delta_train_size + delta_test_size) \n",
    "\n",
    "            # compute remaining\n",
    "            remaining = (train_size + test_size) - sum([ sum(split) for split in split_classes_col_dict.values() ])\n",
    "\n",
    "        # assert that distribution meets target split requirements\n",
    "        assert (train_size + test_size) == sum([ sum(split_lst) for split_lst in split_classes_col_dict.values() ])\n",
    "\n",
    "        dataset_mapping_splits[f\"train_{train_size}_test_{test_size}\"] = { \"splits\": split_classes_col_dict, \"config\": { \"train_size\": train_size, \"test_size\": test_size }}\n",
    "\n",
    "    # return the mapping for creating the dataset splits\n",
    "    return classes_dict, dataset_mapping_splits\n",
    "\n",
    "    # AIDs: [Binary]\n",
    "    #               [100, 20] & [1000, 200] & [1600, 400]\n",
    "    #                50s 10s     500s 100s     800s 200s\n",
    "    # Fingerprints: [15 Classes]\n",
    "    #                &  & \n",
    "    #                10s  2s     70s  10s      110s  30s\n",
    "\n",
    "def create_dataset_splits(dataset_name: str = \"AIDS\", pde: bool = False):\n",
    "    classes_dict, dataset_splits = find_dataset_splits(dataset_name=dataset_name, pde=pde)\n",
    "    for dataset_split in dataset_splits:\n",
    "        train_data, test_data = [], []\n",
    "        for class_id in dataset_splits[dataset_split][\"splits\"]:\n",
    "            class_train_size, class_test_size = dataset_splits[dataset_split][\"splits\"][class_id]\n",
    "            random.shuffle(classes_dict[class_id]) # shuffle data first before distr.\n",
    "            for i in range(class_train_size + class_test_size):\n",
    "                if i < class_train_size:\n",
    "                    train_data.append(classes_dict[class_id][i])\n",
    "                else:\n",
    "                    test_data.append(classes_dict[class_id][i])\n",
    "\n",
    "        # struct to create\n",
    "        struct = {\n",
    "            \"raw\": {\n",
    "                \"x_train_data\": [],\n",
    "                \"y_train_data\": [],\n",
    "                \"x_test_data\" : [],\n",
    "                \"y_test_data\" : []\n",
    "            },\n",
    "            \"geometric\": {\n",
    "                \"qgcn_train_data\": deepcopy(train_data),\n",
    "                \"qgcn_test_data\" : deepcopy(test_data),\n",
    "                \"sgcn_train_data\": deepcopy(train_data),\n",
    "                \"sgcn_test_data\" : deepcopy(test_data),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # save file\n",
    "        parent_dirpath = os.path.join(\".\", \"Dataset\", \"Raw\", \"Generated\")\n",
    "        filename = f\"{dataset_split}_struct_{dataset_name.strip().lower()}_graph_data_pde={ 'yes' if pde else 'no' }.pkl\"\n",
    "        rel_full_path = os.path.join(parent_dirpath, filename)\n",
    "        with open(rel_full_path, \"wb\") as pickle_file_handle:\n",
    "            pickle.dump(struct, pickle_file_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "struct = {\n",
    "    \"raw\": {\n",
    "        \"x_train_data\": [],\n",
    "        \"y_train_data\": [],\n",
    "        \"x_test_data\" : [],\n",
    "        \"y_test_data\" : []\n",
    "    },\n",
    "    \"geometric\": {\n",
    "        \"qgcn_train_data\":  [],\n",
    "        \"qgcn_test_data\" : [],\n",
    "        \"sgcn_train_data\": [],\n",
    "        \"sgcn_test_data\" :  [],\n",
    "    }\n",
    "}\n",
    "\n",
    "# dataset_name=\"AIDS\", pde: bool = False -> 2 classes 400, 1600\n",
    "# {0: 400, 1: 1600}\n",
    "# dataset_name=\"Fingerprints\", pde: bool = False -> 2 classes 400, 1600\n",
    "# {0: 369, 1: 136, 2: 496, 3: 75, 4: 396, 5: 368, 6: 134, 7: 4, 8: 105, 9: 18, 10: 20, 11: 20, 12: 2, 13: 4, 14: 2}\n",
    "# order_dataset_into_classes(read_dataset())\n",
    "\n",
    "# # dataset with pde=OFF\n",
    "# create_dataset_splits(dataset_name=\"AIDS\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"COIL-DEL\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"ENZYMES\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Fingerprint\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"FRANKENSTEIN\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Letter-high\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Letter-low\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Letter-med\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"MUTAG\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Mutagenicity\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"PROTEINS\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"PROTEINS-Full\", pde=False)\n",
    "# create_dataset_splits(dataset_name=\"Synthie\", pde=False)\n",
    "\n",
    "# dataset with pde=ON\n",
    "create_dataset_splits(dataset_name=\"AIDS\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"COIL-DEL\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"ENZYMES\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Fingerprint\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"FRANKENSTEIN\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Letter-high\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Letter-low\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Letter-med\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"MUTAG\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Mutagenicity\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"PROTEINS\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"PROTEINS-Full\", pde=True)\n",
    "create_dataset_splits(dataset_name=\"Synthie\", pde=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsb1LlU3qWyzcb4f21+gvE",
   "collapsed_sections": [
    "pTvlgrVn1rq-",
    "hQHMrFvBKfSM",
    "pVC2ljkyNdKe",
    "RNKuPi8zRovH",
    "TdprOTLOJjL8",
    "Yb3Kbn2l40Ed",
    "q64pDw-JP9mY",
    "lYBZ00NUyC5_",
    "9sXZFgKhHPtk",
    "hvuCCFLIbHqB"
   ],
   "mount_file_id": "1CZX3GaZ1vszAJm7-31BafQDL3y2vTcF7",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "qgcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
