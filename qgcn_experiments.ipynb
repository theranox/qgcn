{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARD DATASETS \n",
    "###### Run the below cell to generate results for paper\n",
    "###### NOTE: Paper generated results 1 run at a time and and used a post processing script to average out the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "W9CA2vEPHNRl",
    "outputId": "56a0077a-43f2-4cce-e6ee-9f2baf1915ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "\n",
    "from cnn.cnn_architectures import CNN\n",
    "from sgcn.src.architectures import SGCN\n",
    "from qgcn.qgcn_architectures import QGCN\n",
    "from experiment import Experiment\n",
    "# from gcn_architectures import Custom_GCN, GCN, GraphConvCN, GATConvCN, SGConvCN\n",
    "\n",
    "# Empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# current file directory\n",
    "current_file_dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "# device type for all models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using cuda: \", torch.cuda.is_available(), \"device\")\n",
    "\n",
    "# define the different datasets supported in sweep with their respective model config params\n",
    "dataset_mapping = {\n",
    "    \"mnist\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 1,\n",
    "        \"dataset_name\": \"mnist_dataset_selfloops_True_edgeft_None_norm_True\",\n",
    "        \"self_loops_included\": True,\n",
    "        \"layers_num\": 3,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 3 * 3, # same size as CNN kernel size\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "    \"fashionmnist\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 1,\n",
    "        \"dataset_name\": \"fashionmnist_dataset_selfloops_True_edgeft_None_norm_True\",\n",
    "        \"self_loops_included\": True,\n",
    "        \"layers_num\": 6,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 3 * 3, # same size as CNN kernel size\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 3,\n",
    "        \"dataset_name\": \"cifar10_dataset_selfloops_True_edgeft_None_norm_True\",\n",
    "        \"self_loops_included\": True,\n",
    "        \"layers_num\": 9,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 3 * 3, # same size as CNN kernel size\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Define sweep parameters\n",
    "\"\"\"\n",
    "datasets = [ {\"train\":100,\"test\":20,\"batch_size\":16}, \n",
    "             {\"train\":1000,\"test\":200,\"batch_size\":16}, \n",
    "             {\"train\":10000,\"test\":1000,\"batch_size\":16} ]\n",
    "lrs    = [10,  5,   1,   0.5,  0.1,  0.05,   0.01,  0.005,  0.001, 0.0001, 0.00001, 0.00005, 0.000001]      \n",
    "epochs = [100, 100, 150, 150,  300,  300,    400,   400,    500,   600,    800,     800,     800]\n",
    "runs   = [3,   3,   3,   3,    3,    3,      3,     3,      3,     3,      3,       3,       3]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper function for collating results\n",
    "Define function for handling collation\n",
    "\"\"\"\n",
    "def collate_stats(stats_name, max_stats, smoothened_stats):\n",
    "  # collate the results to cache\n",
    "  collated_stats_keys = [ f\"{stats_name}_max_of_maxs\",\n",
    "                          f\"{stats_name}_avg_of_maxs\",\n",
    "                          f\"{stats_name}_std_of_maxs\",\n",
    "                          f\"{stats_name}_max_of_smaxs\",\n",
    "                          f\"{stats_name}_avg_of_smaxs\",\n",
    "                          f\"{stats_name}_std_of_smaxs\"  ]\n",
    "  cnn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "  sgcn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "  qgcn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "\n",
    "  # save the results\n",
    "  cnn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"cnn\"]) <= 1) else statistics.stdev(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"cnn\"]) <= 1) else statistics.stdev(smoothened_stats[\"cnn\"]), 5)\n",
    "\n",
    "  sgcn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"sgcn\"]) <= 1) else statistics.stdev(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"sgcn\"]) <= 1) else statistics.stdev(smoothened_stats[\"sgcn\"]), 5)\n",
    "\n",
    "  qgcn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"qgcn\"]) <= 1) else statistics.stdev(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"qgcn\"]) <= 1) else statistics.stdev(smoothened_stats[\"qgcn\"]), 5)\n",
    "\n",
    "  # Return results\n",
    "  return cnn_collated_results, sgcn_collated_results, qgcn_collated_results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SWEEPING Logic below\n",
    "Loops through the different sweep parameters to train different models\n",
    "\"\"\"\n",
    "for dataset_split in datasets: # loop over datasets\n",
    "    # extract batch size which is peculiar to dataset split\n",
    "    batch_size = dataset_split.get('batch_size', 64)\n",
    "    \n",
    "    # Inner loop goes over all datasets\n",
    "    for selected_dataset, selected_dataset_config in dataset_mapping.items():\n",
    "        # Prep experiment name\n",
    "        experiment_name = f\"BATCH-RESULTS-ALL-DATASETS-{selected_dataset.capitalize()}_Summary\"\n",
    "        experiments_dir = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"Experiments\")\n",
    "        experiment_result_filepath = os.path.join(experiments_dir, f'{\"_\".join(experiment_name.split(\" \"))}.yaml')\n",
    "        averaging_window_width = 0.05 # fraction -> 5%\n",
    "        results = {} # to hold results for saving\n",
    "        if os.path.exists(experiment_result_filepath):\n",
    "            with open(experiment_result_filepath, \"r\") as file_stream:\n",
    "                results = yaml.safe_load(file_stream)\n",
    "                if results:\n",
    "                    results = dict(results)\n",
    "                else:\n",
    "                    results = {}\n",
    "                    \n",
    "        # Load the required params for this dataset\n",
    "        dataset_name           = selected_dataset_config[\"dataset_name\"]\n",
    "        self_loops_included    = selected_dataset_config[\"self_loops_included\"]\n",
    "        layers_num             = selected_dataset_config[\"layers_num\"]\n",
    "        model_dim              = selected_dataset_config[\"model_dim\"]\n",
    "        out_channels_1         = selected_dataset_config[\"out_channels_1\"]\n",
    "        dim_coor               = selected_dataset_config[\"dim_coor\"]\n",
    "        label_dim              = selected_dataset_config[\"label_dim\"]\n",
    "        out_dim                = selected_dataset_config[\"out_dim\"]\n",
    "        in_channels            = selected_dataset_config[\"in_channels\"]\n",
    "        hidden_channels        = selected_dataset_config[\"hidden_channels\"]\n",
    "        out_channels           = selected_dataset_config[\"out_channels\"]\n",
    "        cnn_kernel_size        = selected_dataset_config[\"cnn_kernel_size\"]\n",
    "        cnn_stride             = selected_dataset_config[\"cnn_stride\"]\n",
    "        cnn_padding            = selected_dataset_config[\"cnn_padding\"]\n",
    "        qgcn_num_subkernels    = selected_dataset_config[\"qgcn_num_subkernels\"]\n",
    "        is_dataset_homogenous  = selected_dataset_config[\"is_dataset_homogenous\"]\n",
    "    \n",
    "        # Inner-Inner loop\n",
    "        for i, lr in enumerate(lrs): # loop over learning rates\n",
    "          optim_params = { \"lr\": lr }\n",
    "          num_epochs = epochs[i]\n",
    "          num_runs = runs[i]\n",
    "          # create the key for hashing into results\n",
    "          results_hash_key = f'train_{dataset_split[\"train\"]}_test_{dataset_split[\"test\"]}_lr_{lr}'\n",
    "          results[results_hash_key] = {}\n",
    "          # run stats\n",
    "          mean_train_loss = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_train_loss = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          max_train_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_train_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          max_test_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_test_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          for run in range(num_runs): # loop over num runs\n",
    "            # initialize the models\n",
    "            sgcn_model = SGCN(dim_coor=dim_coor,\n",
    "                              out_dim=out_dim,\n",
    "                              input_features=in_channels, # label_dim,\n",
    "                              layers_num=layers_num,\n",
    "                              model_dim=model_dim,\n",
    "                              out_channels_1=out_channels_1)\n",
    "\n",
    "            # cnn model\n",
    "            cnn_model = CNN(out_dim=out_dim,\n",
    "                            hidden_channels=hidden_channels,\n",
    "                            in_channels=in_channels, \n",
    "                            out_channels=out_channels,\n",
    "                            kernel_size=cnn_kernel_size,\n",
    "                            stride=cnn_stride,\n",
    "                            layers_num=layers_num,\n",
    "                            padding=cnn_padding)\n",
    "\n",
    "            # qgcn model\n",
    "            qgcn_model = QGCN(dim_coor=dim_coor,\n",
    "                              out_dim=out_dim,\n",
    "                              in_channels=in_channels,\n",
    "                              hidden_channels=hidden_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              layers_num=layers_num,\n",
    "                              num_kernels=qgcn_num_subkernels,\n",
    "                              self_loops_included=self_loops_included,\n",
    "                              is_dataset_homogenous=is_dataset_homogenous, # determines whether to apply caching for kernel masks\n",
    "                              apply_spatial_scalars=False, # SGCN-like behavior; refer to code and paper for more details\n",
    "                              initializer_model=cnn_model, # comment this out to have independent initializations\n",
    "                              device=device)\n",
    "\n",
    "            # setup experiments to run\n",
    "            num_train, num_test = dataset_split[\"train\"], dataset_split[\"test\"]\n",
    "            experiment_id = f'_{run}_full_blown_exp_train_{num_train}_test_{num_test}_i_{i}_lr_{lr}_num_epochs_{num_epochs}'\n",
    "            experiment = Experiment(sgcn_model = sgcn_model,\n",
    "                                    qgcn_model = qgcn_model,\n",
    "                                    cnn_model = cnn_model,\n",
    "                                    optim_params = optim_params,\n",
    "                                    base_path = \"./\", \n",
    "                                    num_train = num_train,\n",
    "                                    num_test = num_test,\n",
    "                                    dataset_name = dataset_name,\n",
    "                                    train_batch_size = batch_size,\n",
    "                                    test_batch_size = batch_size,\n",
    "                                    train_shuffle_data = True,\n",
    "                                    test_shuffle_data = False,\n",
    "                                    id = experiment_id) # mark this experiment ...\n",
    "\n",
    "            # run the experiment ...\n",
    "            experiment.run(num_epochs=num_epochs, eval_training_set=False) # specify num epochs ...\n",
    "\n",
    "            # load collected stats during runs ...\n",
    "            (train_cnn_loss_array, train_qgcn_loss_array, train_sgcn_loss_array, \\\n",
    "             train_cnn_acc_array, train_qgcn_acc_array, train_sgcn_acc_array, \\\n",
    "             test_cnn_acc_array, test_qgcn_acc_array, test_sgcn_acc_array) = experiment.load_cached_results() # only accuracies on train and test sets ...\n",
    "            \n",
    "            # get the mean stats\n",
    "            mean_train_loss[\"cnn\"].append(statistics.mean(train_cnn_loss_array))\n",
    "            mean_train_loss[\"sgcn\"].append(statistics.mean(train_sgcn_loss_array))\n",
    "            mean_train_loss[\"qgcn\"].append(statistics.mean(train_qgcn_loss_array))\n",
    "            \n",
    "            max_train_acc[\"cnn\"].append(max(train_cnn_acc_array))\n",
    "            max_train_acc[\"sgcn\"].append(max(train_sgcn_acc_array))\n",
    "            max_train_acc[\"qgcn\"].append(max(train_qgcn_acc_array))\n",
    "            \n",
    "            max_test_acc[\"cnn\"].append(max(test_cnn_acc_array))\n",
    "            max_test_acc[\"sgcn\"].append(max(test_sgcn_acc_array))\n",
    "            max_test_acc[\"qgcn\"].append(max(test_qgcn_acc_array))\n",
    "\n",
    "            # get the smoothened max test acc\n",
    "            num_averaging_window = int(math.ceil(averaging_window_width * num_epochs))\n",
    "            smoothened_train_loss[\"cnn\"].append(statistics.mean(train_cnn_loss_array[-num_averaging_window:]))\n",
    "            smoothened_train_loss[\"sgcn\"].append(statistics.mean(train_sgcn_loss_array[-num_averaging_window:]))\n",
    "            smoothened_train_loss[\"qgcn\"].append(statistics.mean(train_qgcn_loss_array[-num_averaging_window:]))\n",
    "            \n",
    "            smoothened_train_acc[\"cnn\"].append(statistics.mean(train_cnn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_train_acc[\"sgcn\"].append(statistics.mean(train_sgcn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_train_acc[\"qgcn\"].append(statistics.mean(train_qgcn_acc_array[-num_averaging_window:]))\n",
    "            \n",
    "            smoothened_test_acc[\"cnn\"].append(statistics.mean(test_cnn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_test_acc[\"sgcn\"].append(statistics.mean(test_sgcn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_test_acc[\"qgcn\"].append(statistics.mean(test_qgcn_acc_array[-num_averaging_window:]))\n",
    "\n",
    "          # get collated stats\n",
    "          train_loss_cnn_results, train_loss_sgcn_results, train_loss_qgcn_results = collate_stats(\"train_loss\", mean_train_loss, smoothened_train_loss)\n",
    "          train_acc_cnn_results,  train_acc_sgcn_results,  train_acc_qgcn_results  = collate_stats(\"train_acc\", max_train_acc, smoothened_train_acc)\n",
    "          test_acc_cnn_results,   test_acc_sgcn_results,   test_acc_qgcn_results   = collate_stats(\"test_acc\", max_test_acc, smoothened_test_acc)\n",
    "          all_cnn_stats  = {**train_loss_cnn_results,  **train_acc_cnn_results,  **test_acc_cnn_results}\n",
    "          all_sgcn_stats = {**train_loss_sgcn_results, **train_acc_sgcn_results, **test_acc_sgcn_results}\n",
    "          all_qgcn_stats = {**train_loss_qgcn_results, **train_acc_qgcn_results, **test_acc_qgcn_results}\n",
    "\n",
    "          # save results into results obj\n",
    "          results[results_hash_key][\"cnn\"] = all_cnn_stats\n",
    "          results[results_hash_key][\"sgcn\"] = all_sgcn_stats\n",
    "          results[results_hash_key][\"qgcn\"] = all_qgcn_stats\n",
    "\n",
    "          # pickle the results\n",
    "          with open(experiment_result_filepath, \"w\") as file_stream:\n",
    "            yaml.dump(results, file_stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM DATASETS \n",
    "###### Run the below cell to generate results for paper\n",
    "###### NOTE: Paper generated results 1 run at a time and used a post processing script to average out the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "\n",
    "from cnn.cnn_architectures import CNN\n",
    "from sgcn.src.architectures import SGCN\n",
    "from qgcn.qgcn_architectures import QGCN\n",
    "from experiment import Experiment\n",
    "# from gcn_architectures import Custom_GCN, GCN, GraphConvCN, GATConvCN, SGConvCN\n",
    "\n",
    "# Empty cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# current file directory\n",
    "current_file_dirpath = os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "# device type for all models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"using cuda: \", torch.cuda.is_available(), \"device\")\n",
    "\n",
    "# define the different datasets supported in sweep with their respective model config params\n",
    "dataset_mapping = {\n",
    "    \"navier_stokes_binary\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 1,\n",
    "        \"dataset_name\": \"navier_stokes_binary\",\n",
    "        \"self_loops_included\": False,\n",
    "        \"layers_num\": 3,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 11,\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "    \"navier_stokes_denary_1\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 1,\n",
    "        \"dataset_name\": \"navier_stokes_denary_1\",\n",
    "        \"self_loops_included\": False,\n",
    "        \"layers_num\": 3,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 11,\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "    \"navier_stokes_denary_2\": {\n",
    "        \"label_dim\": 1,\n",
    "        \"out_dim\": 10,\n",
    "        \"dim_coor\": 2,\n",
    "        \"in_channels\": 1,\n",
    "        \"dataset_name\": \"navier_stokes_denary_2\",\n",
    "        \"self_loops_included\": False,\n",
    "        \"layers_num\": 3,\n",
    "        \"model_dim\": 32,\n",
    "        \"out_channels_1\": 64,\n",
    "        \"hidden_channels\": 32,\n",
    "        \"out_channels\": 64,\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_stride\": 1,\n",
    "        \"cnn_padding\": 1,\n",
    "        \"qgcn_num_subkernels\": 11,\n",
    "        \"is_dataset_homogenous\": True # Homogenous means spatial location mask of the nodes do not change\n",
    "    },\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Define sweep parameters\n",
    "\"\"\"\n",
    "datasets = [ {\"train\":100,\"test\":20,\"batch_size\":16}, \n",
    "             {\"train\":1000,\"test\":200,\"batch_size\":16}, \n",
    "             {\"train\":10000,\"test\":1000,\"batch_size\":16} ]\n",
    "lrs    = [10,  5,   1,   0.5,  0.1,  0.05,   0.01,  0.005,  0.001, 0.0001, 0.00001, 0.00005, 0.000001]      \n",
    "epochs = [100, 100, 100, 100,  100,  100,    100,   100,    100,   100,    100,     100,     100]\n",
    "runs   = [3,   3,   3,   3,    3,    3,      3,     3,      3,     3,      3,       3,       3]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Helper function for collating results\n",
    "Define function for handling collation\n",
    "\"\"\"\n",
    "def collate_stats(stats_name, max_stats, smoothened_stats):\n",
    "  # collate the results to cache\n",
    "  collated_stats_keys = [ f\"{stats_name}_max_of_maxs\",\n",
    "                          f\"{stats_name}_avg_of_maxs\",\n",
    "                          f\"{stats_name}_std_of_maxs\",\n",
    "                          f\"{stats_name}_max_of_smaxs\",\n",
    "                          f\"{stats_name}_avg_of_smaxs\",\n",
    "                          f\"{stats_name}_std_of_smaxs\"  ]\n",
    "  cnn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "  sgcn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "  qgcn_collated_results = { x: 0 for x in collated_stats_keys} \n",
    "\n",
    "  # save the results\n",
    "  cnn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"cnn\"]) <= 1) else statistics.stdev(max_stats[\"cnn\"]), 5)\n",
    "  cnn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"cnn\"]) <= 1) else statistics.stdev(smoothened_stats[\"cnn\"]), 5)\n",
    "\n",
    "  sgcn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"sgcn\"]) <= 1) else statistics.stdev(max_stats[\"sgcn\"]), 5)\n",
    "  sgcn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"sgcn\"]) <= 1) else statistics.stdev(smoothened_stats[\"sgcn\"]), 5)\n",
    "\n",
    "  qgcn_collated_results[f\"{stats_name}_max_of_maxs\"]  = round(max(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_max_of_smaxs\"] = round(max(smoothened_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_avg_of_maxs\"]  = round(statistics.mean(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_avg_of_smaxs\"] = round(statistics.mean(smoothened_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_std_of_maxs\"]  = round(0 if (len(max_stats[\"qgcn\"]) <= 1) else statistics.stdev(max_stats[\"qgcn\"]), 5)\n",
    "  qgcn_collated_results[f\"{stats_name}_std_of_smaxs\"] = round(0 if (len(smoothened_stats[\"qgcn\"]) <= 1) else statistics.stdev(smoothened_stats[\"qgcn\"]), 5)\n",
    "\n",
    "  # Return results\n",
    "  return cnn_collated_results, sgcn_collated_results, qgcn_collated_results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SWEEPING Logic below\n",
    "Loops through the different sweep parameters to train different models\n",
    "\"\"\"\n",
    "for dataset_split in datasets: # loop over datasets\n",
    "    # extract batch size which is peculiar to dataset split\n",
    "    batch_size = dataset_split.get('batch_size', 64)\n",
    "    \n",
    "    # Inner loop goes over all datasets\n",
    "    for selected_dataset, selected_dataset_config in dataset_mapping.items():\n",
    "        # Prep experiment name\n",
    "        experiment_name = f\"BATCH-RESULTS-ALL-DATASETS-{selected_dataset.capitalize()}_Summary\"\n",
    "        experiments_dir = os.path.join(os.path.dirname(os.path.realpath('__file__')), \"Experiments\")\n",
    "        experiment_result_filepath = os.path.join(experiments_dir, f'{\"_\".join(experiment_name.split(\" \"))}.yaml')\n",
    "        averaging_window_width = 0.05 # fraction -> 5%\n",
    "        results = {} # to hold results for saving\n",
    "        if os.path.exists(experiment_result_filepath):\n",
    "            with open(experiment_result_filepath, \"r\") as file_stream:\n",
    "                results = yaml.safe_load(file_stream)\n",
    "                if results:\n",
    "                    results = dict(results)\n",
    "                else:\n",
    "                    results = {}\n",
    "                    \n",
    "        # Load the required params for this dataset\n",
    "        dataset_name           = selected_dataset_config[\"dataset_name\"]\n",
    "        self_loops_included    = selected_dataset_config[\"self_loops_included\"]\n",
    "        layers_num             = selected_dataset_config[\"layers_num\"]\n",
    "        model_dim              = selected_dataset_config[\"model_dim\"]\n",
    "        out_channels_1         = selected_dataset_config[\"out_channels_1\"]\n",
    "        dim_coor               = selected_dataset_config[\"dim_coor\"]\n",
    "        label_dim              = selected_dataset_config[\"label_dim\"]\n",
    "        out_dim                = selected_dataset_config[\"out_dim\"]\n",
    "        in_channels            = selected_dataset_config[\"in_channels\"]\n",
    "        hidden_channels        = selected_dataset_config[\"hidden_channels\"]\n",
    "        out_channels           = selected_dataset_config[\"out_channels\"]\n",
    "        cnn_kernel_size        = selected_dataset_config[\"cnn_kernel_size\"]\n",
    "        cnn_stride             = selected_dataset_config[\"cnn_stride\"]\n",
    "        cnn_padding            = selected_dataset_config[\"cnn_padding\"]\n",
    "        qgcn_num_subkernels    = selected_dataset_config[\"qgcn_num_subkernels\"]\n",
    "        is_dataset_homogenous  = selected_dataset_config[\"is_dataset_homogenous\"]\n",
    "    \n",
    "        # Inner-Inner loop\n",
    "        for i, lr in enumerate(lrs): # loop over learning rates\n",
    "          optim_params = { \"lr\": lr }\n",
    "          num_epochs = epochs[i]\n",
    "          num_runs = runs[i]\n",
    "          # create the key for hashing into results\n",
    "          results_hash_key = f'train_{dataset_split[\"train\"]}_test_{dataset_split[\"test\"]}_lr_{lr}'\n",
    "          results[results_hash_key] = {}\n",
    "          # run stats\n",
    "          mean_train_loss = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_train_loss = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          max_train_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_train_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          max_test_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          smoothened_test_acc = { \"cnn\": [], \"sgcn\": [], \"qgcn\": []}\n",
    "          for run in range(num_runs): # loop over num runs\n",
    "            # initialize the models\n",
    "            sgcn_model = SGCN(dim_coor=dim_coor,\n",
    "                              out_dim=out_dim,\n",
    "                              input_features=in_channels, # label_dim,\n",
    "                              layers_num=layers_num,\n",
    "                              model_dim=model_dim,\n",
    "                              out_channels_1=out_channels_1)\n",
    "\n",
    "            # cnn model\n",
    "            cnn_model = None\n",
    "\n",
    "            # convolutional graph neural networks\n",
    "            qgcn_model = QGCN(dim_coor=dim_coor,\n",
    "                              out_dim=out_dim,\n",
    "                              in_channels=in_channels,\n",
    "                              hidden_channels=hidden_channels,\n",
    "                              out_channels=out_channels,\n",
    "                              layers_num=layers_num,\n",
    "                              num_kernels=qgcn_num_subkernels,\n",
    "                              self_loops_included=self_loops_included,\n",
    "                              is_dataset_homogenous=is_dataset_homogenous, # determines whether to apply caching for kernel masks\n",
    "                              apply_spatial_scalars=False, # SGCN-like behavior; refer to code and paper for more details\n",
    "                              initializer_model=cnn_model, # comment this out to have independent initializations\n",
    "                              device=device)\n",
    "            \n",
    "            # setup experiments to run\n",
    "            num_train, num_test = dataset_split[\"train\"], dataset_split[\"test\"]\n",
    "            experiment_id = f'_{run}_full_blown_exp_train_{num_train}_test_{num_test}_i_{i}_lr_{lr}_num_epochs_{num_epochs}'\n",
    "            experiment = Experiment(sgcn_model = sgcn_model,\n",
    "                                    qgcn_model = qgcn_model,\n",
    "                                    cnn_model = cnn_model,\n",
    "                                    optim_params = optim_params,\n",
    "                                    base_path = \"./\", \n",
    "                                    num_train = num_train,\n",
    "                                    num_test = num_test,\n",
    "                                    dataset_name = dataset_name,\n",
    "                                    train_batch_size=batch_size,\n",
    "                                    test_batch_size=batch_size,\n",
    "                                    train_shuffle_data=True,\n",
    "                                    test_shuffle_data=False,\n",
    "                                    id = experiment_id) # mark this experiment ...\n",
    "\n",
    "            # run the experiment ...\n",
    "            experiment.run(num_epochs=num_epochs, eval_training_set=False) # specify num epochs ...\n",
    "\n",
    "            # load collected stats during runs ...\n",
    "            (train_cnn_loss_array, train_qgcn_loss_array, train_sgcn_loss_array, \\\n",
    "             train_cnn_acc_array, train_qgcn_acc_array, train_sgcn_acc_array, \\\n",
    "             test_cnn_acc_array, test_qgcn_acc_array, test_sgcn_acc_array) = experiment.load_cached_results() # only accuracies on train and test sets ...\n",
    "            \n",
    "            # get the mean stats\n",
    "            mean_train_loss[\"cnn\"].append(statistics.mean(train_cnn_loss_array))\n",
    "            mean_train_loss[\"sgcn\"].append(statistics.mean(train_sgcn_loss_array))\n",
    "            mean_train_loss[\"qgcn\"].append(statistics.mean(train_qgcn_loss_array))\n",
    "            \n",
    "            max_train_acc[\"cnn\"].append(max(train_cnn_acc_array))\n",
    "            max_train_acc[\"sgcn\"].append(max(train_sgcn_acc_array))\n",
    "            max_train_acc[\"qgcn\"].append(max(train_qgcn_acc_array))\n",
    "            \n",
    "            max_test_acc[\"cnn\"].append(max(test_cnn_acc_array))\n",
    "            max_test_acc[\"sgcn\"].append(max(test_sgcn_acc_array))\n",
    "            max_test_acc[\"qgcn\"].append(max(test_qgcn_acc_array))\n",
    "\n",
    "            # get the smoothened max test acc\n",
    "            num_averaging_window = int(math.ceil(averaging_window_width * num_epochs))\n",
    "            smoothened_train_loss[\"cnn\"].append(statistics.mean(train_cnn_loss_array[-num_averaging_window:]))\n",
    "            smoothened_train_loss[\"sgcn\"].append(statistics.mean(train_sgcn_loss_array[-num_averaging_window:]))\n",
    "            smoothened_train_loss[\"qgcn\"].append(statistics.mean(train_qgcn_loss_array[-num_averaging_window:]))\n",
    "            \n",
    "            smoothened_train_acc[\"cnn\"].append(statistics.mean(train_cnn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_train_acc[\"sgcn\"].append(statistics.mean(train_sgcn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_train_acc[\"qgcn\"].append(statistics.mean(train_qgcn_acc_array[-num_averaging_window:]))\n",
    "            \n",
    "            smoothened_test_acc[\"cnn\"].append(statistics.mean(test_cnn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_test_acc[\"sgcn\"].append(statistics.mean(test_sgcn_acc_array[-num_averaging_window:]))\n",
    "            smoothened_test_acc[\"qgcn\"].append(statistics.mean(test_qgcn_acc_array[-num_averaging_window:]))\n",
    "          \n",
    "          # get collated stats\n",
    "          train_loss_cnn_results, train_loss_sgcn_results, train_loss_qgcn_results = collate_stats(\"train_loss\", mean_train_loss, smoothened_train_loss)\n",
    "          train_acc_cnn_results,  train_acc_sgcn_results,  train_acc_qgcn_results  = collate_stats(\"train_acc\", max_train_acc, smoothened_train_acc)\n",
    "          test_acc_cnn_results,   test_acc_sgcn_results,   test_acc_qgcn_results   = collate_stats(\"test_acc\", max_test_acc, smoothened_test_acc)\n",
    "          all_cnn_stats  = {**train_loss_cnn_results,  **train_acc_cnn_results,  **test_acc_cnn_results}\n",
    "          all_sgcn_stats = {**train_loss_sgcn_results, **train_acc_sgcn_results, **test_acc_sgcn_results}\n",
    "          all_qgcn_stats = {**train_loss_qgcn_results, **train_acc_qgcn_results, **test_acc_qgcn_results}\n",
    "          \n",
    "          # save results into results obj\n",
    "          results[results_hash_key][\"cnn\"] = all_cnn_stats\n",
    "          results[results_hash_key][\"sgcn\"] = all_sgcn_stats\n",
    "          results[results_hash_key][\"qgcn\"] = all_qgcn_stats\n",
    "\n",
    "          # pickle the results\n",
    "          with open(experiment_result_filepath, \"w\") as file_stream:\n",
    "            yaml.dump(results, file_stream)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Plots weight distribution histogram\n",
    "- Usecase: confirming equivalence between CNN and QGCN param inits\n",
    "\"\"\"\n",
    "def plot_weight_histogram(w_tensor):\n",
    "    randNumpy = w_tensor.cpu().detach().clone().numpy().flatten()\n",
    "    n_bins = 10\n",
    "    fig, ax0 = plt.subplots(nrows=1, ncols=1)\n",
    "\n",
    "    colors = ['black']\n",
    "    ax0.hist(randNumpy, n_bins, density=True, histtype='bar', color=colors, label=colors)\n",
    "    ax0.legend(prop={'size': 10})\n",
    "    ax0.set_title('bars with legend')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract model params from all three models:\n",
    "def print_model_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        plot_weight_histogram(param)\n",
    "    print()\n",
    "print_model_params(cnn_model)\n",
    "print_model_params(qgcn_model)\n",
    "print_model_params(sgcn_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOsb1LlU3qWyzcb4f21+gvE",
   "collapsed_sections": [
    "pTvlgrVn1rq-",
    "hQHMrFvBKfSM",
    "pVC2ljkyNdKe",
    "RNKuPi8zRovH",
    "TdprOTLOJjL8",
    "Yb3Kbn2l40Ed",
    "q64pDw-JP9mY",
    "lYBZ00NUyC5_",
    "9sXZFgKhHPtk",
    "hvuCCFLIbHqB"
   ],
   "mount_file_id": "1CZX3GaZ1vszAJm7-31BafQDL3y2vTcF7",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
